{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcASkl02PeHTdgKagrTgTO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patience275/recipebot/blob/main/recipie_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1bs6MayU7xNp"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core  --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU 'langchain[mistralai]'"
      ],
      "metadata": {
        "id": "CbEMKEsaz0NF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import needed libraries\n",
        "import os\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# 2. Set your API key (make sure you saved it in Colab beforehand)\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"MN22VAvXK3a5y0PZggCOx7xJFN4gZ74L\"\n",
        "\n",
        "# 3. Create a simple chat model\n",
        "llm = ChatMistralAI(model=\"mistral-tiny\")\n",
        "\n",
        "# 4. Make a test call\n",
        "response = llm.invoke(\"Give me a simple 2-ingredient recipe.\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lBY51nz0KNL",
        "outputId": "00540bca-eb70-455f-c06b-483fbe8f5c7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here's a simple recipe for banana oatmeal cookies:\n",
            "\n",
            "Ingredients:\n",
            "1. 2 ripe bananas\n",
            "2. 1 cup of old-fashioned oats\n",
            "\n",
            "Instructions:\n",
            "1. Preheat your oven to 350°F (175°C) and line a baking sheet with parchment paper.\n",
            "2. In a large bowl, mash the ripe bananas until they are smooth.\n",
            "3. Add the oats to the mashed bananas and mix until well combined.\n",
            "4. Scoop out spoonfuls of the mixture onto the prepared baking sheet, spacing them about 2 inches apart.\n",
            "5. Bake for 12-15 minutes, or until the edges are golden brown.\n",
            "6. Let the cookies cool on the baking sheet for 5 minutes before transferring them to a wire rack to cool completely.\n",
            "\n",
            "Enjoy your delicious and healthy banana oatmeal cookies!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "memory=ConversationSummaryMemory(llm=llm, return_messages=True,memory_key='chat_history')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8bf8ca-f40e-447f-db39-94f2bc3d2fba",
        "id": "tIZr5fy5sy8U"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-346278106.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationSummaryMemory(llm=llm, return_messages=True,memory_key='chat_history')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "prompt=ChatPromptTemplate([\n",
        "    ('system','you are an excellent chef. using the {ingredients} provided by the user, create a delicious meal. include a catchy name, utensils to use, and the steps to follow to create the meal you suggest'),\n",
        "    MessagesPlaceholder(variable_name ='chat_history'),\n",
        "     ('user','{ingredients}')\n",
        "])"
      ],
      "metadata": {
        "id": "tJFRMqOas8_0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain=RunnableParallel(\n",
        "    chat_history=lambda x:memory.chat_memory.messages,\n",
        "    ingredients=RunnablePassthrough() # Change key to 'ingredients' to match prompt\n",
        ") | prompt | llm"
      ],
      "metadata": {
        "id": "cFzHRyU4uMwB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recipie(ingredients):\n",
        "    response = chain.invoke({\"ingredients\": ingredients})\n",
        "    return response.content # Access content using .content\n",
        "\n",
        "# 5. Test it\n",
        "user_ingredients = input(\"Enter the ingredients you have: \")\n",
        "recipe = get_recipie(user_ingredients)\n",
        "print(\"\\n🍽️ Here is your recipe suggestion:\\n\")\n",
        "print(recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vUZReLs2miQ",
        "outputId": "36ee8061-5631-4704-8056-926a450fbe22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the ingredients you have: beans, rice,tomatoes, oil, onions\n",
            "\n",
            "🍽️ Here is your recipe suggestion:\n",
            "\n",
            "Title: Rice 'n' Beans Delight with Tomato Salsa\n",
            "\n",
            "Utensils:\n",
            "1. Large pot\n",
            "2. Medium pot\n",
            "3. Pan for sautéing\n",
            "4. Knife\n",
            "5. Cutting board\n",
            "6. Measuring cups\n",
            "7. Measuring spoons\n",
            "8. Spoon for stirring\n",
            "9. Serving bowl\n",
            "10. Plates\n",
            "\n",
            "Ingredients:\n",
            "- 1 cup uncooked rice\n",
            "- 2 cups water (for cooking rice)\n",
            "- 1 can (15 oz) beans (drained and rinsed)\n",
            "- 1 medium onion (diced)\n",
            "- 2 medium tomatoes (diced)\n",
            "- 2 tbsp oil\n",
            "- Salt and pepper to taste\n",
            "\n",
            "Steps to follow:\n",
            "\n",
            "1. Cook the rice: In a large pot, combine the rice and water. Bring to a boil, then reduce heat to low, cover, and simmer for about 18 minutes or until the rice is tender and the liquid is absorbed. Set aside.\n",
            "\n",
            "2. Prepare the beans: In a medium pot, heat the oil over medium heat. Add the diced onions and sauté until they become translucent. Add the drained and rinsed beans, season with salt and pepper, and stir to combine. Cook for about 5 minutes.\n",
            "\n",
            "3. Make the tomato salsa: In a separate pan, sauté the diced tomatoes over medium heat until they soften and release their juices. Season with salt and pepper to taste.\n",
            "\n",
            "4. To serve, scoop the cooked rice onto plates, top with the bean mixture, and spoon the tomato salsa over the top. Enjoy your Rice 'n' Beans Delight with Tomato Salsa!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpyzPzi_4VWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bentoml --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP25AUcn4UwX",
        "outputId": "666e0d1f-aa03-4dcd-861d-eef524396cbc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.4/118.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bentoml --quiet"
      ],
      "metadata": {
        "id": "GGs5df3YFfko"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile service.py\n",
        "\n",
        "import bentoml\n",
        "from bentoml.io import JSON\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "llm = ChatMistralAI(model=\"mistral-tiny\")\n",
        "\n",
        "memory=ConversationSummaryMemory(llm=llm, return_messages=True,memory_key='chat_history')\n",
        "\n",
        "prompt=ChatPromptTemplate([\n",
        "    ('system','you are an excellent chef. using the {ingredients} provided by the user, create a delicious meal. include a catchy name, utensils to use, and the steps to follow to create the meal you suggest'),\n",
        "    MessagesPlaceholder(variable_name ='chat_history'),\n",
        "     ('user','{ingredients}')\n",
        "])\n",
        "\n",
        "chain=RunnableParallel(\n",
        "    chat_history=lambda x:memory.chat_memory.messages,\n",
        "    ingredients=RunnablePassthrough()\n",
        ") | prompt | llm\n",
        "\n",
        "def get_recipie(ingredients):\n",
        "    response = chain.invoke({\"ingredients\": ingredients})\n",
        "    return response.content\n",
        "\n",
        "svc=bentoml.Service('recipie_bot-service')\n",
        "\n",
        "@svc.apis(input=JSON(),output=JSON())\n",
        "def recipie(requests):\n",
        "  ingredients=requests.get('ingredients',\"\")\n",
        "  result=get_recipie(ingredients)\n",
        "  return{'recipie':result}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlD4zOXz5uVe",
        "outputId": "7b5bd4fe-44ca-48a6-f2ef-d7a6ac690a38"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Replace 'service.py' with your file name\n",
        "files.download('service.py')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vMTHr2ttJj5D",
        "outputId": "139fdf4e-bf93-4928-ee63-9bbee41eccd1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9033aa19-8e56-4254-b296-fe80f980a131\", \"service.py\", 1261)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}