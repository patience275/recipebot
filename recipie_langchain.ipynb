{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO00J8LQDKGQHYUfzioUZ4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patience275/recipebot/blob/main/recipie_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1bs6MayU7xNp"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core  --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU 'langchain[mistralai]'"
      ],
      "metadata": {
        "id": "6pgznWP-BgIO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get('MISTRAL_API_KEY'):\n",
        "  os.environ['MISTRAL_API_KEY'] = getpass.getpass('enter api key for mistral: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILIQzNbj8nUd",
        "outputId": "58eadcbe-e267-49da-80c1-a5b1cf5795de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter api key for mistral: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "llm=init_chat_model('mistral-small-latest',model_provider='mistralai')\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "llm.invoke([HumanMessage(content='hi, im colleen')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywE_tjve9nys",
        "outputId": "4e1c5a47-0543-4e3b-83e0-809f9618617d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi, Colleen! 😊 How can I help you today? Let me know if you have any questions or need assistance with something!', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 8, 'total_tokens': 38, 'completion_tokens': 30}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--29b6b5e5-2be7-4911-a1fa-ba5f8594004c-0', usage_metadata={'input_tokens': 8, 'output_tokens': 30, 'total_tokens': 38})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "memory=ConversationSummaryMemory(llm=llm, return_messages=True,memory_key='chat_history')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400b628b-0150-436a-e721-e13b3d78eab1",
        "id": "tIZr5fy5sy8U"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-346278106.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory=ConversationSummaryMemory(llm=llm, return_messages=True,memory_key='chat_history')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "prompt=ChatPromptTemplate([\n",
        "    ('system','you are an excellent chef. using the {ingredients} provided by the user, create a delicious meal. include a catchy name, utensils to use, and the steps to follow to create the meal you suggest'),\n",
        "    MessagesPlaceholder(variable_name ='chat_history'),\n",
        "     ('user','{ingredients}')\n",
        "])"
      ],
      "metadata": {
        "id": "tJFRMqOas8_0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain=RunnableParallel(\n",
        "    chat_history=lambda x:memory.chat_memory.messages,\n",
        "    ingredients=RunnablePassthrough() # Change key to 'ingredients' to match prompt\n",
        ") | prompt | llm"
      ],
      "metadata": {
        "id": "cFzHRyU4uMwB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_ingredients=input('n\\ enter your ingredients(or type\"exit\" to quit)')\n",
        "  if user_ingredients.lower()=='exit':\n",
        "    break\n",
        "  response=chain.invoke(input={'ingredients':user_ingredients})\n",
        "  print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjgTDDqsP4Nq",
        "outputId": "1336c73c-e030-41d9-df94-a6479316f815"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
            "/tmp/ipython-input-1880630427.py:2: SyntaxWarning: invalid escape sequence '\\ '\n",
            "  user_ingredients=input('n\\ enter your ingredients(or type\"exit\" to quit)')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n\\ enter your ingredients(or type\"exit\" to quit)onions,rice, tomatoes\n",
            "content='**Dish Name:** \"Tasty Tomato Rice Pilaf\"\\n\\n**Utensils:**\\n1. Medium-sized pot with a lid\\n2. Cutting board\\n3. Knife\\n4. Wooden spoon\\n5. Measuring cups and spoons\\n6. Colander (for rinsing rice, if necessary)\\n\\n**Steps:**\\n\\n1. **Prepare the rice:**\\n   - Rinse 1 cup of rice under cold water using a colander to remove excess starch. Set aside.\\n\\n2. **Chop the vegetables:**\\n   - Finely chop 1 medium onion.\\n   - Dice 2 medium tomatoes.\\n\\n3. **Cook the onions:**\\n   - Heat 2 tablespoons of oil in the pot over medium heat.\\n   - Add the chopped onions and sauté until they become translucent and slightly golden, about 5 minutes.\\n\\n4. **Add the tomatoes:**\\n   - Stir in the diced tomatoes and cook for about 5 minutes, until they soften and start to break down.\\n\\n5. **Combine the rice:**\\n   - Add the rinsed rice to the pot and stir well to combine with the onion and tomato mixture.\\n   - Cook for 2-3 minutes, stirring occasionally.\\n\\n6. **Add water and seasonings:**\\n   - Pour in 2 cups of water (or broth for extra flavor).\\n   - Add 1 teaspoon of salt, 1/2 teaspoon of black pepper, and any other desired seasonings (e.g., garlic powder, paprika, or dried herbs).\\n   - Stir well to combine.\\n\\n7. **Cook the rice:**\\n   - Bring the mixture to a boil, then reduce the heat to low.\\n   - Cover the pot with a lid and let it simmer for about 15-20 minutes, or until the rice is cooked and the liquid has been absorbed.\\n\\n8. **Rest and fluff:**\\n   - Turn off the heat and let the rice rest, covered, for an additional 5-10 minutes.\\n   - Fluff the rice with a fork and serve hot.\\n\\nEnjoy your \"Tasty Tomato Rice Pilaf\"! This dish pairs well with grilled chicken, fish, or a side salad. You can also customize it with your favorite herbs and spices.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 68, 'total_tokens': 536, 'completion_tokens': 468}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'} id='run--3215fc4e-6960-4457-9364-c9242bc07661-0' usage_metadata={'input_tokens': 68, 'output_tokens': 468, 'total_tokens': 536}\n",
            "n\\ enter your ingredients(or type\"exit\" to quit)exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "1Zzt_uA4XDBv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recipie(ingredients):\n",
        "  return chain.invoke({'ingredients': ingredients}).content # Access the content attribute\n",
        "\n",
        "import gradio as gr\n",
        "chatbot=gr.Interface(\n",
        " fn=generate_recipie,\n",
        " inputs=gr.Textbox(lines=4,placeholder='e.g:tomatoes,onions,eggs', label='your ingredients'), # label moved inside gr.Textbox\n",
        " outputs=gr.Textbox(label='generated recipie'), # label moved inside gr.Textbox\n",
        " title='your awesome recipie generator',\n",
        " description='eneter your ingredients ypu have  and get a delicious recipie,tyhe utensils to use and step by step instructions'\n",
        ")\n",
        "chatbot.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "2a6c376c-e6cc-4774-b4fd-ea0caaaf30e3",
        "id": "xqz_d6REZDYv"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8140f8e1c6e01d1801.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8140f8e1c6e01d1801.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0JHFAqNzqc3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile recipie.py\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get('MISTRAL_API_KEY'):\n",
        "  os.environ['MISTRAL_API_KEY'] = getpass.getpass('enter api key for mistral: ')\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "llm=init_chat_model('mistral-small-latest',model_provider='mistralai')\n",
        "\n",
        "\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "memory=ConversationSummaryMemory(llm=llm, return_messages=True,memory_key='chat_history')\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "prompt=ChatPromptTemplate([\n",
        "    ('system','you are an excellent chef. using the {ingredients} provided by the user, create a delicious meal. include a catchy name, utensils to use, and the steps to follow to create the meal you suggest'),\n",
        "    MessagesPlaceholder(variable_name ='chat_history'),\n",
        "     ('user','{ingredients}')\n",
        "])\n",
        "\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain=RunnableParallel(\n",
        "    chat_history=lambda x:memory.chat_memory.messages,\n",
        "    ingredients=RunnablePassthrough() # Change key to 'ingredients' to match prompt\n",
        ") | prompt | llm\n",
        "\n",
        "import gradio as gr\n",
        "def generate_recipie(ingredients):\n",
        "  return chain.invoke({'ingredients': ingredients}).content # Access the content attribute\n",
        "\n",
        "import gradio as gr\n",
        "chatbot=gr.Interface(\n",
        " fn=generate_recipie,\n",
        " inputs=gr.Textbox(lines=4,placeholder='e.g:tomatoes,onions,eggs', label='your ingredients'), # label moved inside gr.Textbox\n",
        " outputs=gr.Textbox(label='generated recipie'), # label moved inside gr.Textbox\n",
        " title='your awesome recipie generator',\n",
        " description='enter your ingredients you have  and get a delicious recipie,tyhe utensils to use and step by step instructions'\n",
        ")\n",
        "chatbot.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AErbj-WxzqRX",
        "outputId": "2ffb4caf-a4fa-48e5-dbe1-11046da346cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing recipie.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "gradio\n",
        "langchain-core\n",
        "langchain\n",
        "mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOYOZ4lo1aGM",
        "outputId": "f8359bf2-17c0-4698-a522-74562b3c360b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    }
  ]
}